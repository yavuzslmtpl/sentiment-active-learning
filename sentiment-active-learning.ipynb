{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1730372951507,
     "user": {
      "displayName": "Yavuz Selim TEPELİ",
      "userId": "08614579116546635472"
     },
     "user_tz": -180
    },
    "id": "-r5amL67uHzQ"
   },
   "source": [
    "import json\n",
    "import random\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from keras.layers import Embedding, Flatten, Dense, Conv1D, MaxPooling1D, SimpleRNN, LSTM, TextVectorization\n",
    "from keras.losses import CategoricalCrossentropy\n",
    "from keras.models import Sequential\n",
    "from keras.utils import plot_model\n",
    "from scipy import sparse\n",
    "from scipy.stats import entropy\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from tqdm import tqdm"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1730372956368,
     "user": {
      "displayName": "Yavuz Selim TEPELİ",
      "userId": "08614579116546635472"
     },
     "user_tz": -180
    },
    "id": "h2YQkkQ7uSgX"
   },
   "source": [
    "random_seed = 41\n",
    "result = {\n",
    "    \"dataset\": \"go_emotions\",  # dbpedia | imdb_reviews | tweet_eval | sst-2 | go_emotions | wine_reviews\n",
    "    \"train_seed\": 43,\n",
    "    \"train_count\": 10000,\n",
    "    \"test_seed\": 43,\n",
    "    \"test_count\": 2000,\n",
    "    \"stopword_removal\": True,  # True | False\n",
    "    \"embedding\": \"tfidf\",  # glove | tfidf\n",
    "    \"model\": \"SVM\",  # CNN | RNN | LSTM | SVM | LR | D-TREE | R-FOREST | NBC\n",
    "    \"seed_size\": 100,\n",
    "    \"batch_size\": 100,\n",
    "    \"num_steps\": 29,\n",
    "    \"al_methods\": [\"rs\", \"lf\", \"lc\", \"ms\", \"es\"],\n",
    "    \"durations\": {},\n",
    "    \"accuracies\": {},\n",
    "    \"f1_scores\": {}\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1730372969847,
     "user": {
      "displayName": "Yavuz Selim TEPELİ",
      "userId": "08614579116546635472"
     },
     "user_tz": -180
    },
    "id": "NpuKgJaxtNvG",
    "outputId": "1758dea1-4b7b-406a-e74e-8567667beef1"
   },
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "print(stopwords)\n",
    "print(len(stopwords))\n",
    "\n",
    "\n",
    "# https://spotintelligence.com/2022/12/10/stop-words-removal\n",
    "def remove_stop_words(sentence, additional_stopwords=None):\n",
    "    # Split the sentence into individual words\n",
    "    if additional_stopwords is None:\n",
    "        additional_stopwords = []\n",
    "    words = sentence.split()\n",
    "    # Use a list comprehension to remove stop words\n",
    "    filtered_words = [word for word in words if word not in stopwords and word not in additional_stopwords]\n",
    "    # Join the filtered words back into a sentence\n",
    "    return ' '.join(filtered_words)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1730212610537,
     "user": {
      "displayName": "Yavuz Selim TEPELİ",
      "userId": "08614579116546635472"
     },
     "user_tz": -180
    },
    "id": "vHfhh7ldtNvG"
   },
   "source": [
    "# https://huggingface.co/datasets/DeveloperOats/DBPedia_Classes\n",
    "def dataset_dbpedia():\n",
    "    print(\"Loading DBPedia Dataset\")\n",
    "    dataset = load_dataset(\"DeveloperOats/DBPedia_Classes\")\n",
    "    dbpedia_train = dataset['train'].shuffle(seed=result[\"train_seed\"])\n",
    "    dbpedia_train_x = np.array(dbpedia_train['text'][:result[\"train_count\"]])\n",
    "    dbpedia_train_y = np.array(dbpedia_train['l1'][:result[\"train_count\"]])\n",
    "    dbpedia_test = dataset['test'].shuffle(seed=result[\"test_seed\"])\n",
    "    dbpedia_test_x = np.array(dbpedia_test['text'][:result[\"test_count\"]])\n",
    "    dbpedia_test_y = np.array(dbpedia_test['l1'][:result[\"test_count\"]])\n",
    "\n",
    "    # Remove stopwords\n",
    "    if result[\"stopword_removal\"]:\n",
    "        print(\"Stopwords Removing\")\n",
    "        dbpedia_train_x = [remove_stop_words(i) for i in dbpedia_train_x]\n",
    "        dbpedia_test_x = [remove_stop_words(i) for i in dbpedia_test_x]\n",
    "    return dbpedia_train_x, dbpedia_train_y, dbpedia_test_x, dbpedia_test_y"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1730212610538,
     "user": {
      "displayName": "Yavuz Selim TEPELİ",
      "userId": "08614579116546635472"
     },
     "user_tz": -180
    },
    "id": "t0sam38CtNvG"
   },
   "source": [
    "# https://huggingface.co/datasets/stanfordnlp/imdb\n",
    "def dataset_imdb_reviews():\n",
    "    print(\"Loading Imdb Reviews Dataset\")\n",
    "    dataset = load_dataset(\"stanfordnlp/imdb\")\n",
    "    imdb_train = dataset['train'].shuffle(seed=result[\"train_seed\"])\n",
    "    imdb_train_x = np.array(imdb_train['text'][:result[\"train_count\"]])\n",
    "    imdb_train_y = np.array(imdb_train['label'][:result[\"train_count\"]])\n",
    "    imdb_test = dataset['test'].shuffle(seed=result[\"test_seed\"])\n",
    "    imdb_test_x = np.array(imdb_test['text'][:result[\"test_count\"]])\n",
    "    imdb_test_y = np.array(imdb_test['label'][:result[\"test_count\"]])\n",
    "\n",
    "    imdb_train_x = [i.replace(\"<br />\", \"\") for i in imdb_train_x]\n",
    "    imdb_test_x = [i.replace(\"<br />\", \"\") for i in imdb_test_x]\n",
    "\n",
    "    # Remove stopwords\n",
    "    if result[\"stopword_removal\"]:\n",
    "        print(\"Stopwords Removing\")\n",
    "        imdb_train_x = [remove_stop_words(i) for i in imdb_train_x]\n",
    "        imdb_test_x = [remove_stop_words(i) for i in imdb_test_x]\n",
    "    return imdb_train_x, imdb_train_y, imdb_test_x, imdb_test_y"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "executionInfo": {
     "elapsed": 0,
     "status": "ok",
     "timestamp": 1730212610539,
     "user": {
      "displayName": "Yavuz Selim TEPELİ",
      "userId": "08614579116546635472"
     },
     "user_tz": -180
    },
    "id": "jKCMJz8itNvH"
   },
   "source": [
    "# https://huggingface.co/datasets/cardiffnlp/tweet_eval\n",
    "def dataset_tweet_eval():\n",
    "    print(\"Loading Tweet Eval Dataset\")\n",
    "    dataset = load_dataset(\"tweet_eval\", \"emotion\")\n",
    "    tweet_train = dataset['train'].shuffle(seed=result[\"train_seed\"])\n",
    "    tweet_train_x = np.array(tweet_train['text'][:result[\"train_count\"]])\n",
    "    tweet_train_y = np.array(tweet_train['label'][:result[\"train_count\"]])\n",
    "    tweet_test = dataset['test'].shuffle(seed=result[\"test_seed\"])\n",
    "    tweet_test_x = np.array(tweet_test['text'][:result[\"test_count\"]])\n",
    "    tweet_test_y = np.array(tweet_test['label'][:result[\"test_count\"]])\n",
    "    # Remove stopwords\n",
    "    if result[\"stopword_removal\"]:\n",
    "        print(\"Stopwords Removing\")\n",
    "        tweet_train_x = [remove_stop_words(i, [\"@user\"]) for i in tweet_train_x]\n",
    "        tweet_test_x = [remove_stop_words(i, [\"@user\"]) for i in tweet_test_x]\n",
    "    return tweet_train_x, tweet_train_y, tweet_test_x, tweet_test_y"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# https://huggingface.co/datasets/stanfordnlp/sst2\n",
    "def dataset_sst2():\n",
    "    print(\"Loading SST-2 Dataset\")\n",
    "    dataset = load_dataset(\"stanfordnlp/sst2\")\n",
    "    sst2_train = dataset['train'].shuffle(seed=result[\"train_seed\"])\n",
    "    sst2_train_x = np.array(sst2_train['sentence'][:result[\"train_count\"]])\n",
    "    sst2_train_y = np.array(sst2_train['label'][:result[\"train_count\"]])\n",
    "    sst2_test = dataset['validation'].shuffle(seed=result[\"test_seed\"])\n",
    "    sst2_test_x = np.array(sst2_test['sentence'][:result[\"test_count\"]])\n",
    "    sst2_test_y = np.array(sst2_test['label'][:result[\"test_count\"]])\n",
    "    # Remove stopwords\n",
    "    if result[\"stopword_removal\"]:\n",
    "        print(\"Stopwords Removing\")\n",
    "        sst2_train_x = [remove_stop_words(i) for i in sst2_train_x]\n",
    "        sst2_test_x = [remove_stop_words(i) for i in sst2_test_x]\n",
    "    return sst2_train_x, sst2_train_y, sst2_test_x, sst2_test_y"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# google-research-datasets/go_emotions\n",
    "def dataset_go_emotions():\n",
    "    print(\"Loading Go Emotions Dataset\")\n",
    "    dataset = load_dataset(\"google-research-datasets/go_emotions\", \"simplified\")\n",
    "    dataset = dataset.filter(lambda x: len(x['labels']) == 1)\n",
    "    go_emotions_train = dataset['train'].shuffle(seed=result[\"train_seed\"])\n",
    "    go_emotions_train_x = np.array(go_emotions_train['text'][:result[\"train_count\"]])\n",
    "    go_emotions_train_y = np.array([i[0] for i in go_emotions_train['labels']][:result[\"train_count\"]])\n",
    "    go_emotions_test = dataset['test'].shuffle(seed=result[\"test_seed\"])\n",
    "    go_emotions_test_x = np.array(go_emotions_test['text'][:result[\"test_count\"]])\n",
    "    go_emotions_test_y = np.array([i[0] for i in go_emotions_test['labels']][:result[\"test_count\"]])\n",
    "    # Remove stopwords\n",
    "    if result[\"stopword_removal\"]:\n",
    "        print(\"Stopwords Removing\")\n",
    "        go_emotions_train_x = [remove_stop_words(i) for i in go_emotions_train_x]\n",
    "        go_emotions_test_x = [remove_stop_words(i) for i in go_emotions_test_x]\n",
    "    return go_emotions_train_x, go_emotions_train_y, go_emotions_test_x, go_emotions_test_y"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# https://huggingface.co/datasets/james-burton/wine_reviews\n",
    "def dataset_wine_reviews():\n",
    "    print(\"Loading Wine Reviews Dataset\")\n",
    "    dataset = load_dataset(\"james-burton/wine_reviews\")\n",
    "    go_emotions_train = dataset['train'].shuffle(seed=result[\"train_seed\"])\n",
    "    go_emotions_train_x = np.array(go_emotions_train['description'][:result[\"train_count\"]])\n",
    "    go_emotions_train_y = np.array(go_emotions_train['points'][:result[\"train_count\"]])\n",
    "    go_emotions_test = dataset['test'].shuffle(seed=result[\"test_seed\"])\n",
    "    go_emotions_test_x = np.array(go_emotions_test['description'][:result[\"test_count\"]])\n",
    "    go_emotions_test_y = np.array(go_emotions_test['points'][:result[\"test_count\"]])\n",
    "    # Remove stopwords\n",
    "    if result[\"stopword_removal\"]:\n",
    "        print(\"Stopwords Removing\")\n",
    "        go_emotions_train_x = [remove_stop_words(i) for i in go_emotions_train_x]\n",
    "        go_emotions_test_x = [remove_stop_words(i) for i in go_emotions_test_x]\n",
    "    return go_emotions_train_x, go_emotions_train_y, go_emotions_test_x, go_emotions_test_y"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "executionInfo": {
     "elapsed": 7746,
     "status": "ok",
     "timestamp": 1730374849206,
     "user": {
      "displayName": "Yavuz Selim TEPELİ",
      "userId": "08614579116546635472"
     },
     "user_tz": -180
    },
    "id": "lTjun9_ZtNvH"
   },
   "source": [
    "if result[\"dataset\"] == \"dbpedia\":\n",
    "    train_x, train_y, val_x, val_y = dataset_dbpedia()\n",
    "elif result[\"dataset\"] == \"imdb_reviews\":\n",
    "    train_x, train_y, val_x, val_y = dataset_imdb_reviews()\n",
    "elif result[\"dataset\"] == \"tweet_eval\":\n",
    "    train_x, train_y, val_x, val_y = dataset_tweet_eval()\n",
    "elif result[\"dataset\"] == \"sst-2\":\n",
    "    train_x, train_y, val_x, val_y = dataset_sst2()\n",
    "elif result[\"dataset\"] == \"go_emotions\":\n",
    "    train_x, train_y, val_x, val_y = dataset_go_emotions()\n",
    "elif result[\"dataset\"] == \"wine_reviews\":\n",
    "    train_x, train_y, val_x, val_y = dataset_wine_reviews()\n",
    "else:\n",
    "    raise Exception(\"Unknown Database\")\n",
    "\n",
    "print(\"Shape TrainX:%s , TrainY:%s\" % (np.shape(train_x), np.shape(train_y)))\n",
    "print(\"Shape ValX:%s , ValY:%s\" % (np.shape(val_x), np.shape(val_y)))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1730375662331,
     "user": {
      "displayName": "Yavuz Selim TEPELİ",
      "userId": "08614579116546635472"
     },
     "user_tz": -180
    },
    "id": "UmijvK8Z07Ly"
   },
   "source": [
    "def embedding_layer_glove(train_data, val_data):\n",
    "    print(\"Loading GloVe Embedding\")\n",
    "    result[\"embedding\"] = \"glove\"\n",
    "\n",
    "    embedding_dim = 100  # Embedding each vector dimension value\n",
    "    sequence_length = 100  # This is constant of the sentence word length\n",
    "\n",
    "    # Text Vectorization process for both train and validation data\n",
    "    text_vectorization = TextVectorization(output_sequence_length=sequence_length)\n",
    "    text_vectorization.adapt(train_data)\n",
    "    vocabulary = text_vectorization.get_vocabulary()\n",
    "    word_index = dict(zip(vocabulary, range(len(vocabulary))))\n",
    "    vocabulary_size = len(word_index)\n",
    "    print(\"Vocabulary Size: \", vocabulary_size)\n",
    "    train_x_seq = text_vectorization(train_data).numpy()\n",
    "    val_x_seq = text_vectorization(val_data).numpy()\n",
    "\n",
    "    # Reading All Embedding Vectors\n",
    "    embedding_index = {}\n",
    "    f = open('./glove.6B.100d.txt', encoding='utf8')\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embedding_index[word] = coefs\n",
    "    f.close()\n",
    "    print('GloVe has %s word vectors ' % len(embedding_index))\n",
    "\n",
    "    # Mapping Text Vectorization to Embedding\n",
    "    embedding_size = vocabulary_size + 1\n",
    "    embedding_matrix = np.zeros((embedding_size, embedding_dim))\n",
    "    for word, idx in word_index.items():\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[idx] = embedding_vector\n",
    "\n",
    "    return train_x_seq, val_x_seq, Embedding(embedding_size, embedding_dim, input_length=sequence_length,\n",
    "                                             weights=[embedding_matrix], trainable=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4248,
     "status": "ok",
     "timestamp": 1730375667080,
     "user": {
      "displayName": "Yavuz Selim TEPELİ",
      "userId": "08614579116546635472"
     },
     "user_tz": -180
    },
    "id": "1_-PI7metNvI",
    "outputId": "5aa62936-70c3-4569-b517-0a0a986ad275"
   },
   "source": [
    "if result[\"embedding\"] == \"glove\":\n",
    "    train_X, val_X, embedding_layer = embedding_layer_glove(train_x, val_x)\n",
    "elif result[\"embedding\"] == \"tfidf\":\n",
    "    vectorizer = TfidfVectorizer(sublinear_tf=True, use_idf=True)\n",
    "    train_X = vectorizer.fit_transform(train_x).toarray()\n",
    "    val_X = vectorizer.transform(val_x).toarray()\n",
    "else:\n",
    "    raise Exception(\"Unknown Embedding\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "one_hot_encoder = OneHotEncoder()\n",
    "one_hot_encoder.fit(train_y.reshape(-1, 1))\n",
    "train_Y = one_hot_encoder.transform(train_y.reshape(-1, 1)).toarray()\n",
    "val_Y = one_hot_encoder.transform(val_y.reshape(-1, 1)).toarray()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1730375006447,
     "user": {
      "displayName": "Yavuz Selim TEPELİ",
      "userId": "08614579116546635472"
     },
     "user_tz": -180
    },
    "id": "fva7WEQxtNvI"
   },
   "source": [
    "class Trainer:\n",
    "    def __init__(self, class_count):\n",
    "        self.class_count = class_count\n",
    "\n",
    "    def __create_model(self):\n",
    "        if result[\"model\"] == \"CNN\":\n",
    "            self.model = self.__create_conv_model()\n",
    "        elif result[\"model\"] == \"RNN\":\n",
    "            self.model = self.__create_rnn_model()\n",
    "        elif result[\"model\"] == \"LSTM\":\n",
    "            self.model = self.__create_lstm_model()\n",
    "        elif result[\"model\"] == \"SVM\":\n",
    "            self.model = self.__create_svm_model()\n",
    "        elif result[\"model\"] == \"LR\":\n",
    "            self.model = self.__create_logistic_regression_model()\n",
    "        elif result[\"model\"] == \"D-TREE\":\n",
    "            self.model = self.__create_decision_tree_model()\n",
    "        elif result[\"model\"] == \"R-FOREST\":\n",
    "            self.model = self.__create_random_forest_model()\n",
    "        elif result[\"model\"] == \"NBC\":\n",
    "            self.model = self.__create_naive_bayes_model()\n",
    "        else:\n",
    "            raise Exception(\"Unknown Model\")\n",
    "\n",
    "    def __create_conv_model(self):\n",
    "        print(\"Creating CNN Model\")\n",
    "        model = Sequential()\n",
    "        model.add(embedding_layer)\n",
    "        model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
    "        model.add(MaxPooling1D(pool_size=2))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(20, activation=\"sigmoid\"))\n",
    "        model.add(Dense(self.class_count, activation=\"softmax\"))\n",
    "        model.compile(optimizer=\"Adam\", loss=CategoricalCrossentropy(), metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def __create_rnn_model(self):\n",
    "        print(\"Creating RNN Model\")\n",
    "        model = Sequential()\n",
    "        model.add(embedding_layer)\n",
    "        model.add(SimpleRNN(units=10))\n",
    "        model.add(Dense(self.class_count, activation=\"softmax\"))\n",
    "        model.compile(optimizer=\"Adam\", loss=CategoricalCrossentropy(), metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def __create_lstm_model(self):\n",
    "        print(\"Creating LSTM Model\")\n",
    "        model = Sequential()\n",
    "        model.add(embedding_layer)\n",
    "        model.add(LSTM(units=10))\n",
    "        model.add(Dense(self.class_count, activation=\"softmax\"))\n",
    "        model.compile(optimizer=\"Adam\", loss=CategoricalCrossentropy(), metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def __create_svm_model(self):\n",
    "        print(\"Creating SVM Model\")\n",
    "        model = svm.SVC(kernel='linear', probability=True)\n",
    "        return model\n",
    "\n",
    "    def __create_logistic_regression_model(self):\n",
    "        print(\"Creating LR Model\")\n",
    "        model = LogisticRegression()\n",
    "        return model\n",
    "\n",
    "    def __create_decision_tree_model(self):\n",
    "        print(\"Creating D-TREE Model\")\n",
    "        model = DecisionTreeClassifier(random_state=random_seed)\n",
    "        return model\n",
    "\n",
    "    def __create_random_forest_model(self):\n",
    "        print(\"Creating R-FOREST Model\")\n",
    "        model = RandomForestClassifier(random_state=random_seed)\n",
    "        return model\n",
    "\n",
    "    def __create_naive_bayes_model(self):\n",
    "        print(\"Creating NBC Model\")\n",
    "        model = MultinomialNB()\n",
    "        return model\n",
    "\n",
    "    def train_model(self, X, Y, pool=None):\n",
    "        self.__create_model()\n",
    "        if result[\"model\"] in [\"SVM\", \"LR\", \"D-TREE\", \"R-FOREST\", \"NBC\"]:\n",
    "            return self.train_sklearn(X, Y, pool)\n",
    "        else:\n",
    "            return self.train_keras(X, Y, pool)\n",
    "\n",
    "    def train_keras(self, X, Y, pool=None):\n",
    "        self.model.fit(X, Y, epochs=20, verbose=0)\n",
    "\n",
    "        val_y_classes = [np.argmax(p) for p in val_Y]\n",
    "        val_predict_classes = [np.argmax(p) for p in self.model.predict(val_X)]\n",
    "        val_acc = accuracy_score(val_y_classes, val_predict_classes)\n",
    "        val_f1 = f1_score(val_y_classes, val_predict_classes, average='weighted')\n",
    "        pool_predictions = self.model.predict(pool) if pool is not None else None\n",
    "        return val_acc, val_f1, pool_predictions\n",
    "\n",
    "    def train_sklearn(self, X, Y, pool=None):\n",
    "        sparse_X = sparse.csr_matrix(X)\n",
    "        y_classes = one_hot_encoder.inverse_transform(Y).ravel()\n",
    "        self.model.fit(sparse_X, y_classes)\n",
    "\n",
    "        sparse_val_X = sparse.csr_matrix(val_X)\n",
    "        val_predict = self.model.predict(sparse_val_X)\n",
    "        val_acc = accuracy_score(val_y, val_predict)\n",
    "        val_f1 = f1_score(val_y, val_predict, average='weighted')\n",
    "        pool_predictions = self.model.predict_proba(pool) if pool is not None else None\n",
    "        return val_acc, val_f1, pool_predictions\n",
    "\n",
    "    def plot_model(self):\n",
    "        if result[\"model\"] in [\"SVM\", \"LR\", \"D-TREE\", \"R-FOREST\", \"NBC\"]:\n",
    "            return\n",
    "        plot_model(\n",
    "            self.model,\n",
    "            to_file=\"model.png\",\n",
    "            show_shapes=True,\n",
    "            show_dtype=True,\n",
    "            show_layer_names=True,\n",
    "            rankdir=\"TB\",\n",
    "            expand_nested=False,\n",
    "            dpi=96,\n",
    "            layer_range=None,\n",
    "            show_layer_activations=False,\n",
    "            show_trainable=False,\n",
    "        )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1730375006615,
     "user": {
      "displayName": "Yavuz Selim TEPELİ",
      "userId": "08614579116546635472"
     },
     "user_tz": -180
    },
    "id": "o9UvL1UtfzLC"
   },
   "source": [
    "class Dataset:\n",
    "    def __init__(self, X, Y):\n",
    "        self._X = X\n",
    "        self._Y = Y\n",
    "        self._labeled = np.array([False for _ in range(0, len(self._X))])\n",
    "        self._selections = []\n",
    "        self._prev_predictions = np.zeros(np.shape(Y))\n",
    "        self._long_first_sorted = None\n",
    "\n",
    "    @property\n",
    "    def pool(self):\n",
    "        return self._X\n",
    "\n",
    "    @property\n",
    "    def labeled(self):\n",
    "        return self._labeled\n",
    "\n",
    "    @property\n",
    "    def X(self):\n",
    "        return self._X[self._labeled]\n",
    "\n",
    "    @property\n",
    "    def Y(self):\n",
    "        return self._Y[self._labeled]\n",
    "\n",
    "    @property\n",
    "    def selections(self):\n",
    "        return self._selections\n",
    "\n",
    "    def random_sampling(self, batch_count):\n",
    "        not_labeled = np.where(self._labeled == False)[0]\n",
    "        new_labels = []\n",
    "        while len(new_labels) < batch_count:\n",
    "            r = random.randrange(0, len(not_labeled))\n",
    "            if not_labeled[r] not in new_labels:\n",
    "                new_labels.append(not_labeled[r])\n",
    "        self._labeled[new_labels] = True\n",
    "        self._iteration_selections()\n",
    "\n",
    "    def long_first(self, batch_count):\n",
    "        # Data icerisinde tokenlar rakam olarak tutuluyor. Boolean a çevirip toplanarak token sayısı elde ediliyor\n",
    "        # Data degismedigi icin tekrar tekrar bu islem yapilmiyor\n",
    "        if self._long_first_sorted is None:\n",
    "            self._long_first_sorted = sorted([(sum(p > 0), i) for i, p in enumerate(self._X)], reverse=True)\n",
    "        self._label_batch(self._long_first_sorted, batch_count)\n",
    "        self._iteration_selections()\n",
    "\n",
    "    # Least Confidence\n",
    "    def lc_sampling(self, batch_count, predictions):\n",
    "        # En iyi olasılığa sahip sınıfın olasılığı 1 den çıkarılıyor.\n",
    "        lc = sorted([(1 - p[np.argmax(p)], i) for i, p in enumerate(predictions)], reverse=True)\n",
    "        self._label_batch(lc, batch_count)\n",
    "        self._iteration_selections()\n",
    "\n",
    "    # Margin\n",
    "    def margin_sampling(self, batch_count, predictions):\n",
    "        # En iyi olasılığa sahip sınıfların olasılıkları arasındaki fark alınıyor\n",
    "        ms = sorted([(p[np.argsort(p)[-1]] - p[np.argsort(p)[-2]], i) for i, p in enumerate(predictions)])\n",
    "        self._label_batch(ms, batch_count)\n",
    "        self._iteration_selections()\n",
    "\n",
    "    # Entropy\n",
    "    def entropy_sampling(self, batch_count, predictions):\n",
    "        # Olasılık değerleri için entropy değeri hesaplanıyor\n",
    "        es = sorted([(entropy(p), i) for i, p in enumerate(predictions)], reverse=True)\n",
    "        self._label_batch(es, batch_count)\n",
    "        self._iteration_selections()\n",
    "\n",
    "    def _label_batch(self, sorted_candidates, batch_count):\n",
    "        i = 0\n",
    "        for _, j in sorted_candidates:\n",
    "            if not self._labeled[j]:  #if not already labeled\n",
    "                self._labeled[j] = True\n",
    "                i += 1\n",
    "            if i >= batch_count:\n",
    "                break\n",
    "\n",
    "    def _iteration_selections(self):\n",
    "        all_selections = self.labeled.nonzero()[0]\n",
    "        iteration_selections = [i for i in all_selections if not any(i in s_list for s_list in self.selections)]\n",
    "        self._selections.append(iteration_selections)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "executionInfo": {
     "elapsed": 482,
     "status": "ok",
     "timestamp": 1730375007270,
     "user": {
      "displayName": "Yavuz Selim TEPELİ",
      "userId": "08614579116546635472"
     },
     "user_tz": -180
    },
    "id": "l72k45gHYSou"
   },
   "source": [
    "def write_result_file():\n",
    "    with open('result_v1.json', 'w') as file:\n",
    "        json.dump(result, file)\n",
    "\n",
    "\n",
    "write_result_file()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1730212624360,
     "user": {
      "displayName": "Yavuz Selim TEPELİ",
      "userId": "08614579116546635472"
     },
     "user_tz": -180
    },
    "id": "jrfcohwGtNvJ"
   },
   "source": [
    "def active_learning(query_strategy, seed_size, batch_size, num_steps):\n",
    "    \"\"\"\n",
    "    query_strategy - 'lc' for Least confidence sampling\n",
    "                   - 'ms' for Margin sampling\n",
    "                   - 'es' for Entropy sampling\n",
    "                   - 'rs' for Random sampling\n",
    "                   - 'lf' for Long First sampling\n",
    "    \"\"\"\n",
    "    assert query_strategy in [\"lc\", \"ms\", \"ms2\", \"es\", \"rs\", \"lf\"], \"Unknown query strategy\"\n",
    "    random.seed(random_seed)\n",
    "    start_time = time.time()\n",
    "    accuracies = []\n",
    "    f1_scores = []\n",
    "    class_count = len(np.unique(train_y))\n",
    "    t = Trainer(class_count)  #\n",
    "    d = Dataset(train_X, train_Y)\n",
    "    d.random_sampling(seed_size)\n",
    "    acc, f1, predictions = t.train_model(d.X, d.Y, d.pool)\n",
    "    accuracies.append(acc)\n",
    "    f1_scores.append(f1)\n",
    "    t.plot_model()\n",
    "    for _ in tqdm(range(0, num_steps)):\n",
    "        if query_strategy == \"lc\":\n",
    "            d.lc_sampling(batch_size, predictions)\n",
    "        elif query_strategy == \"ms\":\n",
    "            d.margin_sampling(batch_size, predictions)\n",
    "        elif query_strategy == \"es\":\n",
    "            d.entropy_sampling(batch_size, predictions)\n",
    "        elif query_strategy == \"rs\":\n",
    "            d.random_sampling(batch_size)\n",
    "        elif query_strategy == \"lf\":\n",
    "            d.long_first(batch_size)\n",
    "\n",
    "        acc, f1, predictions = t.train_model(d.X, d.Y, d.pool)\n",
    "        print(\"Accuracy: %f\" % acc)\n",
    "        accuracies.append(acc)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    end_time = time.time()\n",
    "    result[\"accuracies\"][query_strategy] = accuracies\n",
    "    result[\"f1_scores\"][query_strategy] = f1_scores\n",
    "    result[\"durations\"][query_strategy] = end_time - start_time\n",
    "    write_result_file()\n",
    "    return accuracies, d.selections"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1730212624362,
     "user": {
      "displayName": "Yavuz Selim TEPELİ",
      "userId": "08614579116546635472"
     },
     "user_tz": -180
    },
    "id": "cvO_eO6CtNvJ"
   },
   "source": [
    "seed_size = result[\"seed_size\"]\n",
    "batch_size = result[\"batch_size\"]\n",
    "num_steps = result[\"num_steps\"]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "full_class_count = len(np.unique(train_y))\n",
    "full_trainer = Trainer(full_class_count)\n",
    "full_acc, full_f1, ful_prob = full_trainer.train_model(train_X, train_Y)\n",
    "print(\"Full Accuracy: \", full_acc)\n",
    "print(\"Full F1 Score: \", full_f1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 779662,
     "status": "ok",
     "timestamp": 1730213404024,
     "user": {
      "displayName": "Yavuz Selim TEPELİ",
      "userId": "08614579116546635472"
     },
     "user_tz": -180
    },
    "id": "qc58sBbdtNvJ",
    "outputId": "094ff4ca-26c0-4e6e-98e9-90989ff2ad16",
    "scrolled": true
   },
   "source": [
    "random_accuracies, random_selections = active_learning(\"rs\", seed_size, batch_size, num_steps)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 831539,
     "status": "ok",
     "timestamp": 1730215038589,
     "user": {
      "displayName": "Yavuz Selim TEPELİ",
      "userId": "08614579116546635472"
     },
     "user_tz": -180
    },
    "id": "5TCtaqi3hgPr",
    "outputId": "8cda3065-c85b-4784-f5a7-94017d674cc9",
    "scrolled": true
   },
   "source": [
    "lc_accuracies, lc_selections = active_learning(\"lc\", seed_size, batch_size, num_steps)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 902484,
     "status": "ok",
     "timestamp": 1730215941074,
     "user": {
      "displayName": "Yavuz Selim TEPELİ",
      "userId": "08614579116546635472"
     },
     "user_tz": -180
    },
    "id": "N14DR_dThjHn",
    "outputId": "e56cabf4-1bf6-414a-c972-cde3b041d928",
    "scrolled": true
   },
   "source": [
    "ms_accuracies, ms_selections = active_learning(\"ms\", seed_size, batch_size, num_steps)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 989236,
     "status": "ok",
     "timestamp": 1730216930313,
     "user": {
      "displayName": "Yavuz Selim TEPELİ",
      "userId": "08614579116546635472"
     },
     "user_tz": -180
    },
    "id": "ZFry5ekthlu9",
    "outputId": "93dc6b93-8cab-4531-8b84-4628bab71f99",
    "scrolled": true
   },
   "source": [
    "es_accuracies, es_selections = active_learning(\"es\", seed_size, batch_size, num_steps)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 803023,
     "status": "ok",
     "timestamp": 1730214207049,
     "user": {
      "displayName": "Yavuz Selim TEPELİ",
      "userId": "08614579116546635472"
     },
     "user_tz": -180
    },
    "id": "uVZ2DHVltNvJ",
    "outputId": "e3f5399c-e017-47f7-b615-04fd894decd5",
    "scrolled": true
   },
   "source": [
    "longfirst_accuracies, longfirst_selections = active_learning(\"lf\", seed_size, batch_size, num_steps)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 718
    },
    "executionInfo": {
     "elapsed": 286,
     "status": "ok",
     "timestamp": 1730216930624,
     "user": {
      "displayName": "Yavuz Selim TEPELİ",
      "userId": "08614579116546635472"
     },
     "user_tz": -180
    },
    "id": "EWExE_T0hmYO",
    "outputId": "288379f7-639d-402d-d845-96cbbf817967"
   },
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(np.arange(seed_size, seed_size + (num_steps + 1) * batch_size, batch_size), random_accuracies,\n",
    "         color=\"b\", label=\"Random Sampling\")\n",
    "plt.plot(np.arange(seed_size, seed_size + (num_steps + 1) * batch_size, batch_size), lc_accuracies,\n",
    "         color=\"g\", label=\"Least Confidence Sampling\")\n",
    "plt.plot(np.arange(seed_size, seed_size + (num_steps + 1) * batch_size, batch_size), ms_accuracies,\n",
    "         color=\"r\", label=\"Margin Sampling\")\n",
    "plt.plot(np.arange(seed_size, seed_size + (num_steps + 1) * batch_size, batch_size), es_accuracies,\n",
    "         color=\"y\", label=\"Entropy Sampling\")\n",
    "plt.plot(np.arange(seed_size, seed_size + (num_steps + 1) * batch_size, batch_size), longfirst_accuracies,\n",
    "         color=\"#000000\", label=\"Long First Sampling\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.title(\"Active Learning on DBPedia Classes Dataset\")\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Labeled data')\n",
    "plt.grid()"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
